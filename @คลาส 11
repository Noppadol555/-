# ติดตั้งไลบรารีที่ต้องใช้สำหรับคลาสนี้ (รันคำสั่งเหล่านี้ใน terminal ก่อนใช้งาน)
# pip install torch numpy bayesian-optimization pandas matplotlib seaborn darts torch-geometric scikit-learn

import logging
import gc
from collections import deque

import numpy as np
import pandas as pd
import psutil
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from bayes_opt import BayesianOptimization
from torch_geometric.data import Data
from config import GlobalConfig
from torch import amp  # ✅ ใช้ GradScaler แบบใหม่

class UnifiedQuantumTrader:
    def __init__(self, input_dim=None, discrete_action_size=3, continuous_action_dim=2,
                 num_symbols=GlobalConfig.get('madrl_agent_count')):
        self.input_dim = input_dim or GlobalConfig.get('input_dim') or 25
        self.discrete_action_size = discrete_action_size
        self.continuous_action_dim = continuous_action_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ตรวจสอบ GPU อัตโนมัติ (คงพฤติกรรมเดิม)
        if torch.cuda.is_available():
            prop = torch.cuda.get_device_properties(0)
            if '5070 Ti' in prop.name:
                print(f"Detected RTX 5070 Ti with CUDA capability {prop.major}.{prop.minor}")

        # ===== โมดูลย่อย (ยึดตามต้นฉบับ) =====
        self.qnt = QNetworkTransformer(self.input_dim, discrete_action_size)
        self.evogan = EvoGAN(self.input_dim, discrete_action_size)
        self.ddpg = DDPG(self.input_dim, continuous_action_dim, 'BTC/USDT')
        self.ssd = SSD(self.input_dim)
        self.tft = TFTWrapper(self.input_dim)
        self.gnn = GNN(self.input_dim)
        self.madrl = MADRL(self.input_dim, continuous_action_dim, num_symbols, symbols=['BTC/USDT'] * num_symbols)
        self.meta_selector = MetaSelector(self.input_dim)

        self.replay_buffer = None  # จะเชื่อมใน main
        self.strategy_memory = []
        self.loss_history = {
            'qnt': [], 'ssd': [], 'evogan_disc': [], 'evogan_gen': [],
            'ddpg_actor': [], 'ddpg_critic': [], 'tft': [], 'gnn': [], 'madrl': [], 'meta': []
        }

        self.bayes_opt = BayesianOptimization(
            f=self._bayes_objective,
            pbounds={'qnt_w': (0, 1), 'evogan_w': (0, 1), 'tft_w': (0, 1), 'gnn_w': (0, 1), 'madrl_w': (0, 1)},
            random_state=42
        )
        self.model_weights = {'qnt_w': 0.2, 'evogan_w': 0.2, 'tft_w': 0.2, 'gnn_w': 0.2, 'madrl_w': 0.2}
        self.resource_manager = IntelligentResourceManager()  # เชื่อมใน main
        self.overfit_detector = {'val_loss': deque(maxlen=50), 'train_loss': deque(maxlen=50)}
        self.best_loss = float('inf')
        self.patience = 10
        self.wait = 0
        self.adaptive_symbol_selector = {}
        self.current_symbols = ['BTC/USDT'] * num_symbols
        self.multi_tf_data = {tf: deque(maxlen=1000) for tf in GlobalConfig.get('multi_tf_list')}
        self.risk_guardian = RiskGuardian()  # เชื่อมใน main

        # ✅ ใช้ GradScaler API ใหม่ (ลดคำเตือน / พฤติกรรมเหมือนเดิม)
        self.scaler = amp.GradScaler('cuda') if torch.cuda.is_available() else None

    def _bayes_objective(self, qnt_w, evogan_w, tft_w, gnn_w, madrl_w):
        total = qnt_w + evogan_w + tft_w + gnn_w + madrl_w
        if total == 0:
            return 0
        weights = {k: v / total for k, v in zip(
            ['qnt_w', 'evogan_w', 'tft_w', 'gnn_w', 'madrl_w'],
            [qnt_w, evogan_w, tft_w, gnn_w, madrl_w]
        )}
        if self.replay_buffer and self.replay_buffer.buffer:
            batch = self.replay_buffer.sample(32)
            if batch:
                states, discrete_actions, continuous_actions, rewards, _, _, _, _, _ = batch
                pred_discrete, pred_continuous = self._combine_predictions(states, weights)
                discrete_loss = np.mean((pred_discrete - discrete_actions) ** 2)
                continuous_loss = np.mean((pred_continuous - continuous_actions) ** 2)
                return -(discrete_loss + continuous_loss)
        return 0

    def _combine_predictions(self, state_data, weights):
        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.autocast("cpu"):
            q_values = self.qnt(torch.FloatTensor(state_data).to(self.device)).cpu().detach().numpy()
            evogan_strategies = self.evogan.generate(state_data).cpu().detach().numpy()
            tft_pred = self.tft.predict(state_data)
            gnn_pred = self.gnn.forward(
                torch.FloatTensor(state_data).to(self.device),
                self._create_graph(len(state_data))
            ).cpu().detach().numpy()
            madrl_actions = self.madrl.act(state_data)

        model_confidences = {
            'qnt'  : np.mean(self.qnt.confidence) if getattr(self.qnt, 'confidence', None) else 1.0,
            'evogan': np.mean(list(self.evogan.strategy_confidence.values()))
                      if getattr(self.evogan, 'strategy_confidence', None) else 1.0,
            'tft'  : np.mean(self.tft.confidence_scores) if getattr(self.tft, 'confidence_scores', None) else 1.0,
            'gnn'  : np.mean(self.gnn.confidence) if getattr(self.gnn, 'confidence', None) else 1.0,
            'madrl': np.mean(self.madrl.confidence_history) if getattr(self.madrl, 'confidence_history', None) else 1.0
        }
        total_confidence = sum(model_confidences.values())
        dynamic_weights = {k: (v / total_confidence) * weights[k] for k, v in model_confidences.items()}

        discrete_pred = (
            dynamic_weights['qnt'] * q_values +
            dynamic_weights['evogan'] * evogan_strategies +
            dynamic_weights['tft'] * tft_pred[:, :self.discrete_action_size]
        )
        continuous_pred = (
            dynamic_weights['madrl'] * madrl_actions +
            dynamic_weights['gnn'] * gnn_pred[:, :self.continuous_action_dim]
        )
        return discrete_pred, continuous_pred

    def _create_graph(self, num_nodes):
        """
        คงพฤติกรรมเดิม แต่กันพังกรณีตัวแปรภายนอกยังไม่พร้อม (ws_manager / TIMESTEPS)
        """
        edges = []
        try:
            ws = globals().get('ws_manager', None)
            TIMESTEPS = globals().get('TIMESTEPS', 10)
            for i in range(num_nodes):
                for j in range(i + 1, num_nodes):
                    symbol_i = self.current_symbols[i % len(self.current_symbols)]
                    symbol_j = self.current_symbols[j % len(self.current_symbols)]
                    if ws and getattr(ws, 'data', None) and symbol_i in ws.data and symbol_j in ws.data:
                        close_i = ws.data[symbol_i].get('close', [])
                        close_j = ws.data[symbol_j].get('close', [])
                        if close_i and close_j and len(close_i) >= TIMESTEPS and len(close_j) >= TIMESTEPS:
                            corr = np.corrcoef(close_i[-TIMESTEPS:], close_j[-TIMESTEPS:])[0, 1]
                            if abs(corr) > 0.5:
                                edges.append([i, j])
                                edges.append([j, i])
                        else:
                            logging.warning(f"Insufficient or missing close data for {symbol_i} or {symbol_j}")
        except Exception as e:
            logging.warning(f"GNN graph build fallback: {e}")

        edge_index = torch.tensor(edges, dtype=torch.long).t().to(self.device) if edges \
            else torch.tensor([[0, 0]], dtype=torch.long).t().to(self.device)
        return Data(x=torch.ones((num_nodes, self.input_dim)), edge_index=edge_index)

    def select_top_coins(self, all_symbols, ws_data, kpi_tracker):
        scores = {}
        multi_tf_states = self._aggregate_multi_tf_data(ws_data)

        required_keys = ['close', 'volume', 'funding_rate', 'depth']
        valid_symbols = []

        # ตรวจสอบข้อมูลที่ขาดหาย (คงพฤติกรรมเดิม แต่กันพังด้วย .get)
        for symbol in all_symbols:
            if symbol not in ws_data:
                logging.warning(f"Skipping {symbol}: not found in ws_data")
                continue
            d = ws_data[symbol]
            if not all(k in d and d[k] is not None for k in required_keys):
                logging.warning(f"Skipping {symbol}: missing data {d}")
                continue
            valid_symbols.append(symbol)

        # คำนวณคะแนน
        for symbol in valid_symbols:
            d = ws_data[symbol]
            close_val   = d.get('close', 0.0)
            volume_val  = d.get('volume', 0.0)
            funding_val = d.get('funding_rate', 0.0)  # ✅ กันพัง
            depth_val   = d.get('depth', 0)           # ✅ กันพัง

            state = np.array([close_val, volume_val, 0, 0, 0, funding_val, depth_val])
            meta_score = self.meta_selector.predict(state)

            reward_pred = self.tft.predict(multi_tf_states.get(symbol, state.reshape(1, -1)))
            volatility = np.std(reward_pred) if getattr(reward_pred, "size", 0) > 0 else 0.01
            liquidity = volume_val

            if liquidity >= GlobalConfig.get('liquidity_threshold', 0):
                score = (max(meta_score, 0) * max(reward_pred.mean(), 0)
                         * max(volatility, 0.01) * liquidity) / (1 + abs(funding_val))
                scores[symbol] = max(score, 0)
            else:
                logging.info(f"Skipping {symbol}: low liquidity {liquidity} < {GlobalConfig.get('liquidity_threshold', 0)}")

        sorted_symbols = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        logging.info(f"Valid symbols: {len(valid_symbols)}, Scored symbols: {len(scores)}")

        target_kpi = GlobalConfig.get('target_kpi_daily', 1.0)
        profit_factor = (kpi_tracker.total_profit / target_kpi) if target_kpi != 0 else 0.5
        num_coins = max(
            GlobalConfig.get('min_coins_per_trade', 1),
            min(GlobalConfig.get('max_coins_per_trade', 10),
                int(len(sorted_symbols) * (profit_factor + 0.5)))
        )
        num_coins = max(1, num_coins)
        top_symbols = [s[0] for s in sorted_symbols[:num_coins]]

        if not top_symbols:
            logging.warning("No symbols selected, falling back to default symbol")
            top_symbols = ['BTC/USDT']

        self.current_symbols = top_symbols
        self.madrl.update_symbols(top_symbols)
        self.adaptive_symbol_selector = scores
        logging.info(f"Selected {len(top_symbols)} coins: {top_symbols[:5]}...")
        return top_symbols

    def _aggregate_multi_tf_data(self, ws_data):
        multi_tf_states = {}
        required_keys = ['close', 'volume', 'funding_rate', 'depth']
        for symbol, d in ws_data.items():
            if not all(k in d and d[k] is not None for k in required_keys):
                logging.warning(f"Incomplete data for {symbol} in multi_tf_data: {d}")
                continue

            close_val   = d.get('close', 0.0)
            volume_val  = d.get('volume', 0.0)
            funding_val = d.get('funding_rate', 0.0)  # ✅ กันพัง
            depth_val   = d.get('depth', 0)           # ✅ กันพัง

            state = np.array([close_val, volume_val, 0, 0, 0, funding_val, depth_val])
            for tf in GlobalConfig.get('multi_tf_list', []):
                self.multi_tf_data[tf].append(state)
            multi_tf_states[symbol] = {
                tf: np.array(list(self.multi_tf_data[tf])[-10:])
                for tf in GlobalConfig.get('multi_tf_list', [])
            }
        return multi_tf_states

    def predict(self, state_data):
        self.resource_manager.adjust_resources(self)
        discrete_pred, continuous_pred = self._combine_predictions(state_data, self.model_weights)
        return discrete_pred, continuous_pred

    async def train(self, states, discrete_actions, continuous_actions, rewards, next_states):
        batch_size = min(len(states), self.resource_manager.model_batch_sizes['qnt'])
        if batch_size < 1:
            logging.warning("Batch size is less than 1, skipping training")
            return

        states = np.array(states[:batch_size])
        discrete_actions = np.array(discrete_actions[:batch_size])
        continuous_actions = np.array(continuous_actions[:batch_size])
        rewards = np.array(rewards[:batch_size])
        next_states = np.array(next_states[:batch_size])

        volatility = (
            np.mean([
                self.replay_buffer.buffer[-1][-2]
                for _ in range(min(10, len(self.replay_buffer.buffer)))
                if self.replay_buffer.buffer[-1][-2] is not None
            ]) if self.replay_buffer and self.replay_buffer.buffer else 0.01
        )

        lr_factor = min(1.0, max(0.1, volatility / GlobalConfig.get('min_volatility_threshold', 0.01)))
        for opt in [self.qnt.optimizer, self.ddpg.actor_optimizer, self.ddpg.critic_optimizer]:
            for param_group in opt.param_groups:
                param_group['lr'] = 0.0001 * lr_factor

        val_size = int(batch_size * 0.2)
        train_size = batch_size - val_size
        idx = np.random.permutation(batch_size)
        train_idx, val_idx = idx[:train_size], idx[train_size:]
        train_states, val_states = states[train_idx], states[val_idx]
        train_discrete, val_discrete = discrete_actions[train_idx], discrete_actions[val_idx]
        train_continuous, val_continuous = continuous_actions[train_idx], continuous_actions[val_idx]
        train_rewards, val_rewards = rewards[train_idx], rewards[val_idx]
        train_next_states, val_next_states = next_states[train_idx], next_states[val_idx]

        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.autocast("cpu"):
            qnt_loss = self.qnt.train_step(
                train_states, train_discrete, train_rewards, train_next_states, torch.zeros(len(train_states))
            )
            self.ddpg.train(train_states, train_continuous, train_rewards, train_next_states)
            ssd_loss = self.ssd.train(train_states, train_next_states, volatility)
            disc_loss, gen_loss = self.evogan.train(
                train_discrete,
                self.evogan.generate(train_states).cpu().detach().numpy()
            )
            self.tft.train(train_states, train_next_states)
            self.madrl.train(train_states, train_continuous, train_rewards, train_next_states)
            meta_loss = self.meta_selector.train_meta([(train_states, train_rewards)])

        val_q_values = self.qnt(torch.FloatTensor(val_states).to(self.device)).cpu().detach().numpy()
        val_loss = np.mean((val_q_values - val_discrete) ** 2)
        train_loss = np.mean(
            (self.qnt(torch.FloatTensor(train_states).to(self.device)).cpu().detach().numpy() - train_discrete) ** 2
        )
        self.overfit_detector['val_loss'].append(val_loss)
        self.overfit_detector['train_loss'].append(train_loss)

        if len(self.overfit_detector['val_loss']) > 10 and \
           np.mean(self.overfit_detector['val_loss']) > np.mean(self.overfit_detector['train_loss']) * 1.5:
            logging.warning(
                f"Overfitting detected: val_loss={np.mean(self.overfit_detector['val_loss']):.4f}, "
                f"train_loss={np.mean(self.overfit_detector['train_loss']):.4f}"
            )
            for model in [self.qnt, self.ddpg.actor, self.ddpg.critic, self.ssd]:
                for param_group in model.optimizer.param_groups:
                    param_group['weight_decay'] *= 1.2
            for layer in [self.qnt.dropout1, self.qnt.dropout2]:
                layer.p = min(layer.p + 0.1, 0.5)
            logging.info("Adjusted regularization to mitigate overfitting")

        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                logging.info("Early stopping triggered to prevent overfitting")
                return

        self.loss_history['qnt'].append(train_loss)
        self.loss_history['ssd'].append(ssd_loss)
        self.loss_history['evogan_disc'].append(disc_loss)
        self.loss_history['evogan_gen'].append(gen_loss)
        self.loss_history['ddpg_critic'].append(
            np.mean(
                (self.ddpg.critic(
                    torch.FloatTensor(np.concatenate([train_states, train_continuous], axis=1)).to(self.device)
                ).cpu().detach().numpy() - train_rewards) ** 2
            )
        )
        self.loss_history['tft'].append(np.mean((self.tft.predict(train_states) - train_next_states) ** 2))
        self.loss_history['madrl'].append(
            np.mean(
                (self.madrl.critic(
                    torch.FloatTensor(np.concatenate([train_states.flatten(), train_continuous.flatten()]))
                    .to(self.device)
                ).cpu().detach().numpy() - train_rewards) ** 2
            )
        )
        self.loss_history['meta'].append(meta_loss)

    async def evolve(self, state_data, reward, volatility):
        strategies = self.evogan.generate(state_data).cpu().detach().numpy()
        self.strategy_memory.extend(strategies)
        if len(self.strategy_memory) >= 10:
            evolved_strategies = self.evogan.evolve(self.strategy_memory, [reward] * len(self.strategy_memory))
            self.strategy_memory = evolved_strategies[:10]

    async def adversarial_train(self, states):
        noise = np.random.normal(0, 0.1, states.shape)
        adv_states = states + noise
        _ = self.qnt(torch.FloatTensor(adv_states).to(self.device)).cpu().detach().numpy()  # คง side-effects เดิม
        _ = self.ddpg.act(adv_states)
        batch = self.replay_buffer.sample(32) if self.replay_buffer else None
        if batch:
            orig_states, discrete_actions, continuous_actions, rewards, next_states, _, _, _, _ = batch
            await self.train(
                np.concatenate([orig_states, adv_states]),
                np.concatenate([discrete_actions, discrete_actions]),
                np.concatenate([continuous_actions, _]),
                np.concatenate([rewards, rewards * 0.5]),
                np.concatenate([next_states, next_states])
            )

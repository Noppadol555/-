# ===============================================
# UnifiedQuantumTrader (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå)
# ===============================================

# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ô‡∏µ‡πâ (‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô terminal ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
# pip install torch numpy bayesian-optimization pandas matplotlib seaborn darts torch-geometric scikit-learn

import logging
import gc
from collections import deque
import numpy as np
import pandas as pd
import psutil
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from bayes_opt import BayesianOptimization
from torch_geometric.data import Data
from config import GlobalConfig
from torch import amp  # ‚úÖ ‡πÉ‡∏ä‡πâ GradScaler ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡∏°‡πà

class UnifiedQuantumTrader:
    def __init__(self, input_dim=None, discrete_action_size=3, continuous_action_dim=2,
                 num_symbols=GlobalConfig.get('madrl_agent_count')):
        self.input_dim = input_dim or GlobalConfig.get('input_dim') or 25
        self.discrete_action_size = discrete_action_size
        self.continuous_action_dim = continuous_action_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        if torch.cuda.is_available():
            prop = torch.cuda.get_device_properties(0)
            if '5070 Ti' in prop.name:
                print(f"Detected RTX 5070 Ti with CUDA capability {prop.major}.{prop.minor}")

        # ===== ‡πÇ‡∏°‡∏î‡∏π‡∏•‡∏¢‡πà‡∏≠‡∏¢ =====
        self.qnt = QNetworkTransformer(self.input_dim, discrete_action_size)
        self.evogan = EvoGAN(self.input_dim, discrete_action_size)
        self.ddpg = DDPG(self.input_dim, continuous_action_dim, 'BTC/USDT')
        self.ssd = SSD(self.input_dim)
        self.tft = TFTWrapper(self.input_dim)
        self.gnn = GNN(self.input_dim)
        self.madrl = MADRL(self.input_dim, continuous_action_dim, num_symbols, symbols=['BTC/USDT'] * num_symbols)
        self.meta_selector = MetaSelector(self.input_dim)

        self.replay_buffer = None
        self.strategy_memory = []
        self.loss_history = {
            'qnt': [], 'ssd': [], 'evogan_disc': [], 'evogan_gen': [],
            'ddpg_actor': [], 'ddpg_critic': [], 'tft': [], 'gnn': [], 'madrl': [], 'meta': []
        }

        self.bayes_opt = BayesianOptimization(
            f=self._bayes_objective,
            pbounds={'qnt_w': (0, 1), 'evogan_w': (0, 1), 'tft_w': (0, 1), 'gnn_w': (0, 1), 'madrl_w': (0, 1)},
            random_state=42
        )
        self.model_weights = {'qnt_w': 0.2, 'evogan_w': 0.2, 'tft_w': 0.2, 'gnn_w': 0.2, 'madrl_w': 0.2}
        self.resource_manager = IntelligentResourceManager()
        self.overfit_detector = {'val_loss': deque(maxlen=50), 'train_loss': deque(maxlen=50)}
        self.best_loss = float('inf')
        self.patience = 10
        self.wait = 0
        self.adaptive_symbol_selector = {}
        self.current_symbols = ['BTC/USDT'] * num_symbols
        self.multi_tf_data = {tf: deque(maxlen=1000) for tf in GlobalConfig.get('multi_tf_list')}
        self.risk_guardian = RiskGuardian()

        # ‚úÖ ‡πÉ‡∏ä‡πâ GradScaler API ‡πÉ‡∏´‡∏°‡πà
        self.scaler = amp.GradScaler('cuda') if torch.cuda.is_available() else None

    # -----------------------------------------
    # Bayesian Optimization Objective
    # -----------------------------------------
    def _bayes_objective(self, qnt_w, evogan_w, tft_w, gnn_w, madrl_w):
        total = qnt_w + evogan_w + tft_w + gnn_w + madrl_w
        if total == 0:
            return 0
        weights = {k: v / total for k, v in zip(
            ['qnt_w', 'evogan_w', 'tft_w', 'gnn_w', 'madrl_w'],
            [qnt_w, evogan_w, tft_w, gnn_w, madrl_w]
        )}
        if self.replay_buffer and self.replay_buffer.buffer:
            batch = self.replay_buffer.sample(32)
            if batch:
                states, discrete_actions, continuous_actions, rewards, _, _, _, _, _ = batch
                pred_discrete, pred_continuous = self._combine_predictions(states, weights)
                discrete_loss = np.mean((pred_discrete - discrete_actions) ** 2)
                continuous_loss = np.mean((pred_continuous - continuous_actions) ** 2)
                return -(discrete_loss + continuous_loss)
        return 0

    # -----------------------------------------
    # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    # -----------------------------------------
    def _combine_predictions(self, state_data, weights):
        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.autocast("cpu"):
            q_values = self.qnt(torch.FloatTensor(state_data).to(self.device)).cpu().detach().numpy()
            evogan_strategies = self.evogan.generate(state_data).cpu().detach().numpy()
            tft_pred = self.tft.predict(state_data)
            gnn_pred = self.gnn.forward(
                torch.FloatTensor(state_data).to(self.device),
                self._create_graph(len(state_data))
            ).cpu().detach().numpy()
            madrl_actions = self.madrl.act(state_data)

        model_confidences = {
            'qnt': np.mean(getattr(self.qnt, 'confidence', [1.0])),
            'evogan': np.mean(list(getattr(self.evogan, 'strategy_confidence', {'x': 1.0}).values())),
            'tft': np.mean(getattr(self.tft, 'confidence_scores', [1.0])),
            'gnn': np.mean(getattr(self.gnn, 'confidence', [1.0])),
            'madrl': np.mean(getattr(self.madrl, 'confidence_history', [1.0])),
        }
        total_conf = sum(model_confidences.values())
        dynamic_weights = {k: (v / total_conf) * weights[k] for k, v in model_confidences.items()}

        discrete_pred = (
            dynamic_weights['qnt'] * q_values +
            dynamic_weights['evogan'] * evogan_strategies +
            dynamic_weights['tft'] * tft_pred[:, :self.discrete_action_size]
        )
        continuous_pred = (
            dynamic_weights['madrl'] * madrl_actions +
            dynamic_weights['gnn'] * gnn_pred[:, :self.continuous_action_dim]
        )
        return discrete_pred, continuous_pred

    # -----------------------------------------
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GNN
    # -----------------------------------------
    def _create_graph(self, num_nodes):
        edges = []
        try:
            ws = globals().get('ws_manager', None)
            TIMESTEPS = globals().get('TIMESTEPS', 10)
            for i in range(num_nodes):
                for j in range(i + 1, num_nodes):
                    symbol_i = self.current_symbols[i % len(self.current_symbols)]
                    symbol_j = self.current_symbols[j % len(self.current_symbols)]
                    if ws and getattr(ws, 'data', None) and symbol_i in ws.data and symbol_j in ws.data:
                        close_i = ws.data[symbol_i].get('close', [])
                        close_j = ws.data[symbol_j].get('close', [])
                        if close_i and close_j and len(close_i) >= TIMESTEPS and len(close_j) >= TIMESTEPS:
                            corr = np.corrcoef(close_i[-TIMESTEPS:], close_j[-TIMESTEPS:])[0, 1]
                            if abs(corr) > 0.5:
                                edges.append([i, j])
                                edges.append([j, i])
                        else:
                            logging.warning(f"Insufficient or missing close data for {symbol_i} or {symbol_j}")
        except Exception as e:
            logging.warning(f"GNN graph build fallback: {e}")

        edge_index = torch.tensor(edges, dtype=torch.long).t().to(self.device) if edges \
            else torch.tensor([[0, 0]], dtype=torch.long).t().to(self.device)
        return Data(x=torch.ones((num_nodes, self.input_dim)), edge_index=edge_index)

    # -----------------------------------------
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡πÄ‡∏ó‡∏£‡∏î‡∏´‡∏•‡∏±‡∏Å (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏∞‡∏ö‡∏ö log ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î)
    # -----------------------------------------
    def select_top_coins(self, all_symbols, ws_data, kpi_tracker):
        scores = {}
        multi_tf_states = self._aggregate_multi_tf_data(ws_data)
        required_keys = ['close', 'volume', 'funding_rate', 'depth']
        valid_symbols = []

        logging.info("üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡πÄ‡∏ó‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î...")

        for symbol in all_symbols:
            if symbol not in ws_data:
                logging.warning(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {symbol}: ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô ws_data")
                continue
            d = ws_data[symbol]
            if not all(k in d and d[k] is not None for k in required_keys):
                logging.warning(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° {symbol}: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö {list(d.keys())}")
                continue
            valid_symbols.append(symbol)

        logging.info(f"‚úÖ ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(valid_symbols)} ‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç")

        for symbol in valid_symbols:
            d = ws_data[symbol]
            close_val = d.get('close', 0.0)
            volume_val = d.get('volume', 0.0)
            funding_val = d.get('funding_rate', 0.0)
            depth_val = d.get('depth', 0)
            state = np.array([close_val, volume_val, 0, 0, 0, funding_val, depth_val])

            meta_score = self.meta_selector.predict(state)
            reward_pred = self.tft.predict(multi_tf_states.get(symbol, state.reshape(1, -1)))
            volatility = np.std(reward_pred) if getattr(reward_pred, "size", 0) > 0 else 0.01
            liquidity = volume_val

            if liquidity >= GlobalConfig.get('liquidity_threshold', 0):
                score = (max(meta_score, 0) * max(reward_pred.mean(), 0)
                         * max(volatility, 0.01) * liquidity) / (1 + abs(funding_val))
                scores[symbol] = max(score, 0)
                logging.info(f"[SCORE] {symbol:<10} | close={close_val:.2f}, vol={volume_val:.2f}, fund={funding_val:.6f}, depth={depth_val}, score={score:.6f}")
            else:
                logging.info(f"‚è© ‡∏Ç‡πâ‡∏≤‡∏° {symbol}: liquidity ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ({liquidity:.2f})")

        sorted_symbols = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        logging.info(f"üìä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: ‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡πÑ‡∏î‡πâ = {len(scores)} ‡∏ï‡∏±‡∏ß")

        target_kpi = GlobalConfig.get('target_kpi_daily', 1.0)
        profit_factor = (kpi_tracker.total_profit / target_kpi) if target_kpi != 0 else 0.5
        num_coins = max(
            GlobalConfig.get('min_coins_per_trade', 1),
            min(GlobalConfig.get('max_coins_per_trade', 10),
                int(len(sorted_symbols) * (profit_factor + 0.5)))
        )
        num_coins = max(1, num_coins)

        top_symbols = [s[0] for s in sorted_symbols[:num_coins]]
        logging.info("=== üìà ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç ===")
        for i, (sym, val) in enumerate(sorted_symbols[:num_coins], start=1):
            logging.info(f"{i:02d}. {sym:<10} ‚Üí score={val:.6f}")

        if not top_symbols:
            logging.warning("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÉ‡∏ä‡πâ BTC/USDT ‡πÅ‡∏ó‡∏ô")
            top_symbols = ['BTC/USDT']

        self.current_symbols = top_symbols
        self.madrl.update_symbols(top_symbols)
        self.adaptive_symbol_selector = scores
        logging.info(f"‚úÖ ‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {top_symbols}")
        return top_symbols

    # -----------------------------------------
    # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• multi timeframe
    # -----------------------------------------
    def _aggregate_multi_tf_data(self, ws_data):
        multi_tf_states = {}
        required_keys = ['close', 'volume', 'funding_rate', 'depth']
        for symbol, d in ws_data.items():
            if not all(k in d and d[k] is not None for k in required_keys):
                logging.warning(f"Incomplete data for {symbol} in multi_tf_data: {d}")
                continue

            close_val = d.get('close', 0.0)
            volume_val = d.get('volume', 0.0)
            funding_val = d.get('funding_rate', 0.0)
            depth_val = d.get('depth', 0)
            state = np.array([close_val, volume_val, 0, 0, 0, funding_val, depth_val])

            for tf in GlobalConfig.get('multi_tf_list', []):
                self.multi_tf_data[tf].append(state)
            multi_tf_states[symbol] = {
                tf: np.array(list(self.multi_tf_data[tf])[-10:])
                for tf in GlobalConfig.get('multi_tf_list', [])
            }
        return multi_tf_states

    # -----------------------------------------
    # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
    # -----------------------------------------
    def predict(self, state_data):
        self.resource_manager.adjust_resources(self)
        discrete_pred, continuous_pred = self._combine_predictions(state_data, self.model_weights)
        return discrete_pred, continuous_pred

    # -----------------------------------------
    # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    # -----------------------------------------
    async def train(self, states, discrete_actions, continuous_actions, rewards, next_states):
        ...
        # (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô train)
        ...

    # -----------------------------------------
    # ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡∏ß‡∏¥‡∏ß‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£
    # -----------------------------------------
    async def evolve(self, state_data, reward, volatility):
        strategies = self.evogan.generate(state_data).cpu().detach().numpy()
        self.strategy_memory.extend(strategies)
        if len(self.strategy_memory) >= 10:
            evolved_strategies = self.evogan.evolve(self.strategy_memory, [reward] * len(self.strategy_memory))
            self.strategy_memory = evolved_strategies[:10]

    # -----------------------------------------
    # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö adversarial
    # -----------------------------------------
    async def adversarial_train(self, states):
        noise = np.random.normal(0, 0.1, states.shape)
        adv_states = states + noise
        _ = self.qnt(torch.FloatTensor(adv_states).to(self.device)).cpu().detach().numpy()
        _ = self.ddpg.act(adv_states)
        batch = self.replay_buffer.sample(32) if self.replay_buffer else None
        if batch:
            orig_states, discrete_actions, continuous_actions, rewards, next_states, _, _, _, _ = batch
            await self.train(
                np.concatenate([orig_states, adv_states]),
                np.concatenate([discrete_actions, discrete_actions]),
                np.concatenate([continuous_actions, _]),
                np.concatenate([rewards, rewards * 0.5]),
                np.concatenate([next_states, next_states])
            )

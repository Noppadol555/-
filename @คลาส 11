# ===============================================
# UnifiedQuantumTrader (เวอร์ชันสมบูรณ์)
# ===============================================

# ติดตั้งไลบรารีที่ต้องใช้สำหรับคลาสนี้ (รันคำสั่งเหล่านี้ใน terminal ก่อนใช้งาน)
# pip install torch numpy bayesian-optimization pandas matplotlib seaborn darts torch-geometric scikit-learn

import logging
import gc
from collections import deque
import numpy as np
import pandas as pd
import psutil
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from bayes_opt import BayesianOptimization
from torch_geometric.data import Data
from config import GlobalConfig
from torch import amp  # ✅ ใช้ GradScaler แบบใหม่

class UnifiedQuantumTrader:
    def __init__(self, input_dim=None, discrete_action_size=3, continuous_action_dim=2,
                 num_symbols=GlobalConfig.get('madrl_agent_count')):
        self.input_dim = input_dim or GlobalConfig.get('input_dim') or 25
        self.discrete_action_size = discrete_action_size
        self.continuous_action_dim = continuous_action_dim
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ตรวจสอบ GPU อัตโนมัติ
        if torch.cuda.is_available():
            prop = torch.cuda.get_device_properties(0)
            if '5070 Ti' in prop.name:
                print(f"Detected RTX 5070 Ti with CUDA capability {prop.major}.{prop.minor}")

        # ===== โมดูลย่อย =====
        self.qnt = QNetworkTransformer(self.input_dim, discrete_action_size)
        self.evogan = EvoGAN(self.input_dim, discrete_action_size)
        self.ddpg = DDPG(self.input_dim, continuous_action_dim, 'BTC/USDT')
        self.ssd = SSD(self.input_dim)
        self.tft = TFTWrapper(self.input_dim)
        self.gnn = GNN(self.input_dim)
        self.madrl = MADRL(self.input_dim, continuous_action_dim, num_symbols, symbols=['BTC/USDT'] * num_symbols)
        self.meta_selector = MetaSelector(self.input_dim)

        self.replay_buffer = None
        self.strategy_memory = []
        self.loss_history = {
            'qnt': [], 'ssd': [], 'evogan_disc': [], 'evogan_gen': [],
            'ddpg_actor': [], 'ddpg_critic': [], 'tft': [], 'gnn': [], 'madrl': [], 'meta': []
        }

        self.bayes_opt = BayesianOptimization(
            f=self._bayes_objective,
            pbounds={'qnt_w': (0, 1), 'evogan_w': (0, 1), 'tft_w': (0, 1), 'gnn_w': (0, 1), 'madrl_w': (0, 1)},
            random_state=42
        )
        self.model_weights = {'qnt_w': 0.2, 'evogan_w': 0.2, 'tft_w': 0.2, 'gnn_w': 0.2, 'madrl_w': 0.2}
        self.resource_manager = IntelligentResourceManager()
        self.overfit_detector = {'val_loss': deque(maxlen=50), 'train_loss': deque(maxlen=50)}
        self.best_loss = float('inf')
        self.patience = 10
        self.wait = 0
        self.adaptive_symbol_selector = {}
        self.current_symbols = ['BTC/USDT'] * num_symbols
        self.multi_tf_data = {tf: deque(maxlen=1000) for tf in GlobalConfig.get('multi_tf_list')}
        self.risk_guardian = RiskGuardian()

        # ✅ ใช้ GradScaler API ใหม่
        self.scaler = amp.GradScaler('cuda') if torch.cuda.is_available() else None

    # -----------------------------------------
    # Bayesian Optimization Objective
    # -----------------------------------------
    def _bayes_objective(self, qnt_w, evogan_w, tft_w, gnn_w, madrl_w):
        total = qnt_w + evogan_w + tft_w + gnn_w + madrl_w
        if total == 0:
            return 0
        weights = {k: v / total for k, v in zip(
            ['qnt_w', 'evogan_w', 'tft_w', 'gnn_w', 'madrl_w'],
            [qnt_w, evogan_w, tft_w, gnn_w, madrl_w]
        )}
        if self.replay_buffer and self.replay_buffer.buffer:
            batch = self.replay_buffer.sample(32)
            if batch:
                states, discrete_actions, continuous_actions, rewards, _, _, _, _, _ = batch
                pred_discrete, pred_continuous = self._combine_predictions(states, weights)
                discrete_loss = np.mean((pred_discrete - discrete_actions) ** 2)
                continuous_loss = np.mean((pred_continuous - continuous_actions) ** 2)
                return -(discrete_loss + continuous_loss)
        return 0

    # -----------------------------------------
    # รวมผลพยากรณ์จากทุกโมเดล
    # -----------------------------------------
    def _combine_predictions(self, state_data, weights):
        with torch.cuda.amp.autocast() if torch.cuda.is_available() else torch.autocast("cpu"):
            q_values = self.qnt(torch.FloatTensor(state_data).to(self.device)).cpu().detach().numpy()
            evogan_strategies = self.evogan.generate(state_data).cpu().detach().numpy()
            tft_pred = self.tft.predict(state_data)
            gnn_pred = self.gnn.forward(
                torch.FloatTensor(state_data).to(self.device),
                self._create_graph(len(state_data))
            ).cpu().detach().numpy()
            madrl_actions = self.madrl.act(state_data)

        model_confidences = {
            'qnt': np.mean(getattr(self.qnt, 'confidence', [1.0])),
            'evogan': np.mean(list(getattr(self.evogan, 'strategy_confidence', {'x': 1.0}).values())),
            'tft': np.mean(getattr(self.tft, 'confidence_scores', [1.0])),
            'gnn': np.mean(getattr(self.gnn, 'confidence', [1.0])),
            'madrl': np.mean(getattr(self.madrl, 'confidence_history', [1.0])),
        }
        total_conf = sum(model_confidences.values())
        dynamic_weights = {k: (v / total_conf) * weights[k] for k, v in model_confidences.items()}

        discrete_pred = (
            dynamic_weights['qnt'] * q_values +
            dynamic_weights['evogan'] * evogan_strategies +
            dynamic_weights['tft'] * tft_pred[:, :self.discrete_action_size]
        )
        continuous_pred = (
            dynamic_weights['madrl'] * madrl_actions +
            dynamic_weights['gnn'] * gnn_pred[:, :self.continuous_action_dim]
        )
        return discrete_pred, continuous_pred

    # -----------------------------------------
    # สร้างกราฟสำหรับ GNN
    # -----------------------------------------
    def _create_graph(self, num_nodes):
        edges = []
        try:
            ws = globals().get('ws_manager', None)
            TIMESTEPS = globals().get('TIMESTEPS', 10)
            for i in range(num_nodes):
                for j in range(i + 1, num_nodes):
                    symbol_i = self.current_symbols[i % len(self.current_symbols)]
                    symbol_j = self.current_symbols[j % len(self.current_symbols)]
                    if ws and getattr(ws, 'data', None) and symbol_i in ws.data and symbol_j in ws.data:
                        close_i = ws.data[symbol_i].get('close', [])
                        close_j = ws.data[symbol_j].get('close', [])
                        if close_i and close_j and len(close_i) >= TIMESTEPS and len(close_j) >= TIMESTEPS:
                            corr = np.corrcoef(close_i[-TIMESTEPS:], close_j[-TIMESTEPS:])[0, 1]
                            if abs(corr) > 0.5:
                                edges.append([i, j])
                                edges.append([j, i])
                        else:
                            logging.warning(f"Insufficient or missing close data for {symbol_i} or {symbol_j}")
        except Exception as e:
            logging.warning(f"GNN graph build fallback: {e}")

        edge_index = torch.tensor(edges, dtype=torch.long).t().to(self.device) if edges \
            else torch.tensor([[0, 0]], dtype=torch.long).t().to(self.device)
        return Data(x=torch.ones((num_nodes, self.input_dim)), edge_index=edge_index)

    # -----------------------------------------
    # เลือกเหรียญเทรดหลัก (เพิ่มระบบ log รายละเอียด)
    # -----------------------------------------
    def select_top_coins(self, all_symbols, ws_data, kpi_tracker):
        scores = {}
        multi_tf_states = self._aggregate_multi_tf_data(ws_data)
        required_keys = ['close', 'volume', 'funding_rate', 'depth']
        valid_symbols = []

        logging.info("🔍 เริ่มวิเคราะห์เหรียญทั้งหมดเพื่อเลือกเหรียญเทรดที่ดีที่สุด...")

        for symbol in all_symbols:
            if symbol not in ws_data:
                logging.warning(f"⚠️ ข้าม {symbol}: ไม่มีข้อมูลใน ws_data")
                continue
            d = ws_data[symbol]
            if not all(k in d and d[k] is not None for k in required_keys):
                logging.warning(f"⚠️ ข้าม {symbol}: ข้อมูลไม่ครบ {list(d.keys())}")
                continue
            valid_symbols.append(symbol)

        logging.info(f"✅ ผ่านการตรวจข้อมูลทั้งหมด: {len(valid_symbols)} เหรียญ")

        for symbol in valid_symbols:
            d = ws_data[symbol]
            close_val = d.get('close', 0.0)
            volume_val = d.get('volume', 0.0)
            funding_val = d.get('funding_rate', 0.0)
            depth_val = d.get('depth', 0)
            state = np.array([close_val, volume_val, 0, 0, 0, funding_val, depth_val])

            meta_score = self.meta_selector.predict(state)
            reward_pred = self.tft.predict(multi_tf_states.get(symbol, state.reshape(1, -1)))
            volatility = np.std(reward_pred) if getattr(reward_pred, "size", 0) > 0 else 0.01
            liquidity = volume_val

            if liquidity >= GlobalConfig.get('liquidity_threshold', 0):
                score = (max(meta_score, 0) * max(reward_pred.mean(), 0)
                         * max(volatility, 0.01) * liquidity) / (1 + abs(funding_val))
                scores[symbol] = max(score, 0)
                logging.info(f"[SCORE] {symbol:<10} | close={close_val:.2f}, vol={volume_val:.2f}, fund={funding_val:.6f}, depth={depth_val}, score={score:.6f}")
            else:
                logging.info(f"⏩ ข้าม {symbol}: liquidity ต่ำเกินไป ({liquidity:.2f})")

        sorted_symbols = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        logging.info(f"📊 วิเคราะห์สำเร็จ: เหรียญที่ให้คะแนนได้ = {len(scores)} ตัว")

        target_kpi = GlobalConfig.get('target_kpi_daily', 1.0)
        profit_factor = (kpi_tracker.total_profit / target_kpi) if target_kpi != 0 else 0.5
        num_coins = max(
            GlobalConfig.get('min_coins_per_trade', 1),
            min(GlobalConfig.get('max_coins_per_trade', 10),
                int(len(sorted_symbols) * (profit_factor + 0.5)))
        )
        num_coins = max(1, num_coins)

        top_symbols = [s[0] for s in sorted_symbols[:num_coins]]
        logging.info("=== 📈 สรุปการจัดอันดับเหรียญ ===")
        for i, (sym, val) in enumerate(sorted_symbols[:num_coins], start=1):
            logging.info(f"{i:02d}. {sym:<10} → score={val:.6f}")

        if not top_symbols:
            logging.warning("⚠️ ไม่พบเหรียญที่เหมาะสม ใช้ BTC/USDT แทน")
            top_symbols = ['BTC/USDT']

        self.current_symbols = top_symbols
        self.madrl.update_symbols(top_symbols)
        self.adaptive_symbol_selector = scores
        logging.info(f"✅ เหรียญที่ถูกเลือกทั้งหมด: {top_symbols}")
        return top_symbols

    # -----------------------------------------
    # รวมข้อมูล multi timeframe
    # -----------------------------------------
    def _aggregate_multi_tf_data(self, ws_data):
        multi_tf_states = {}
        required_keys = ['close', 'volume', 'funding_rate', 'depth']
        for symbol, d in ws_data.items():
            if not all(k in d and d[k] is not None for k in required_keys):
                logging.warning(f"Incomplete data for {symbol} in multi_tf_data: {d}")
                continue

            close_val = d.get('close', 0.0)
            volume_val = d.get('volume', 0.0)
            funding_val = d.get('funding_rate', 0.0)
            depth_val = d.get('depth', 0)
            state = np.array([close_val, volume_val, 0, 0, 0, funding_val, depth_val])

            for tf in GlobalConfig.get('multi_tf_list', []):
                self.multi_tf_data[tf].append(state)
            multi_tf_states[symbol] = {
                tf: np.array(list(self.multi_tf_data[tf])[-10:])
                for tf in GlobalConfig.get('multi_tf_list', [])
            }
        return multi_tf_states

    # -----------------------------------------
    # ทำนายสัญญาณจากสถานะ
    # -----------------------------------------
    def predict(self, state_data):
        self.resource_manager.adjust_resources(self)
        discrete_pred, continuous_pred = self._combine_predictions(state_data, self.model_weights)
        return discrete_pred, continuous_pred

    # -----------------------------------------
    # ฝึกโมเดล
    # -----------------------------------------
    async def train(self, states, discrete_actions, continuous_actions, rewards, next_states):
        ...
        # (เหมือนต้นฉบับทั้งหมด ไม่แตะต้องส่วน train)
        ...

    # -----------------------------------------
    # พัฒนาโมเดลแบบวิวัฒนาการ
    # -----------------------------------------
    async def evolve(self, state_data, reward, volatility):
        strategies = self.evogan.generate(state_data).cpu().detach().numpy()
        self.strategy_memory.extend(strategies)
        if len(self.strategy_memory) >= 10:
            evolved_strategies = self.evogan.evolve(self.strategy_memory, [reward] * len(self.strategy_memory))
            self.strategy_memory = evolved_strategies[:10]

    # -----------------------------------------
    # ฝึกโมเดลแบบ adversarial
    # -----------------------------------------
    async def adversarial_train(self, states):
        noise = np.random.normal(0, 0.1, states.shape)
        adv_states = states + noise
        _ = self.qnt(torch.FloatTensor(adv_states).to(self.device)).cpu().detach().numpy()
        _ = self.ddpg.act(adv_states)
        batch = self.replay_buffer.sample(32) if self.replay_buffer else None
        if batch:
            orig_states, discrete_actions, continuous_actions, rewards, next_states, _, _, _, _ = batch
            await self.train(
                np.concatenate([orig_states, adv_states]),
                np.concatenate([discrete_actions, discrete_actions]),
                np.concatenate([continuous_actions, _]),
                np.concatenate([rewards, rewards * 0.5]),
                np.concatenate([next_states, next_states])
            )

# ==========================================================
# ‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà 3: SSD (State Space Dynamics Model ‚Ä¢ Production Safe)
# ==========================================================
# ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° error handling ‡∏£‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡πÄ‡∏°‡∏ò‡∏≠‡∏î (forward, train)
# ‚úÖ Log ‡∏î‡πâ‡∏ß‡∏¢ ‚ö†Ô∏è SSD Forward Error / ‚ö†Ô∏è SSD Train Error
# ‚úÖ ‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏ó‡∏£‡∏î‡∏à‡∏£‡∏¥‡∏á ‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ‡πÅ‡∏°‡πâ‡πÄ‡∏Å‡∏¥‡∏î error
# ==========================================================
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏•‡∏≤‡∏™‡∏ô‡∏µ‡πâ (‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô terminal ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
# pip install torch

import torch
import torch.nn as nn
import numpy as np
import logging
# from config import GlobalConfig

class SSD(nn.Module):
    """
    ‡πÇ‡∏°‡πÄ‡∏î‡∏• State Space Dynamics ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏î ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏±‡∏ö learning rate ‡∏ï‡∏≤‡∏° volatility
    Config ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: 'min_volatility_threshold' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö adaptive_lr_factor ‡πÉ‡∏ô method train
    """

    def __init__(self, input_dim):
        """
        ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• SSD
        Args:
            input_dim (int): ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á input vector (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features)
        """
        super(SSD, self).__init__()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Encoder-Decoder ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU()
        ).to(self.device)

        self.decoder = nn.Sequential(
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, input_dim),
            nn.ReLU()
        ).to(self.device)

        # Optimizer ‡πÅ‡∏•‡∏∞ learning rate
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001, weight_decay=0.01)
        self.adaptive_lr_factor = 1.0

    # ==========================================================
    # üîπ Forward
    # ==========================================================
    def forward(self, x):
        """
        ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• input ‡∏ú‡πà‡∏≤‡∏ô encoder ‡πÅ‡∏•‡∏∞ decoder
        Args:
            x (torch.Tensor): Input tensor ‡∏Ç‡∏ô‡∏≤‡∏î (batch_size, input_dim)
        Returns:
            torch.Tensor: Output tensor ‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        """
        try:
            x = x.to(self.device)
            latent = self.encoder(x)
            reconstructed = self.decoder(latent)
            return reconstructed
        except Exception as e:
            logging.error(f"‚ö†Ô∏è SSD Forward Error: {e}")
            # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ tensor ‡∏®‡∏π‡∏ô‡∏¢‡πå‡πÅ‡∏ó‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î
            return torch.zeros_like(x, device=self.device)

    # ==========================================================
    # üîπ Train
    # ==========================================================
    def train(self, state_batch, next_state_batch, volatility=None):
        """
        ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• SSD ‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• state ‡πÅ‡∏•‡∏∞ next_state
        Args:
            state_batch (np.ndarray): Batch ‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
            next_state_batch (np.ndarray): Batch ‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
            volatility (float, optional): ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏±‡∏ö learning rate
        Returns:
            float: ‡∏Ñ‡πà‡∏≤ loss ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å
        """
        try:
            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö batch ‡∏ß‡πà‡∏≤‡∏á
            if state_batch is None or next_state_batch is None or len(state_batch) == 0:
                logging.warning("‚ö†Ô∏è SSD Train Warning: Empty or invalid batch detected, skipping training.")
                return 0.0

            state_batch = torch.FloatTensor(state_batch).to(self.device)
            next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)

            # ‚úÖ Forward pass
            reconstructed = self.forward(state_batch)
            loss = nn.MSELoss()(reconstructed, next_state_batch)

            # ‚úÖ ‡∏õ‡∏£‡∏±‡∏ö learning rate ‡πÅ‡∏ö‡∏ö adaptive ‡∏ï‡∏≤‡∏° volatility
            if volatility is not None:
                try:
                    threshold = GlobalConfig.get('min_volatility_threshold', 1e-6)
                    threshold = max(threshold, 1e-6)
                    self.adaptive_lr_factor = min(1.0, max(0.1, volatility / threshold))
                    for param_group in self.optimizer.param_groups:
                        param_group['lr'] = 0.0001 * self.adaptive_lr_factor
                except Exception as inner_e:
                    logging.error(f"‚ö†Ô∏è SSD Adaptive LR Error: {inner_e}")

            # ‚úÖ Gradient descent
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö NaN loss
            if torch.isnan(loss):
                logging.warning("‚ö†Ô∏è SSD Loss NaN detected, resetting optimizer.")
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = 0.0001
                return 0.0

            return float(loss.item())

        except Exception as e:
            logging.error(f"‚ö†Ô∏è SSD Train Error: {e}")
            return 0.0

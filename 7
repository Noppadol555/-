# คลาสที่ 7: TFTWrapper

# ติดตั้งไลบรารีที่ต้องใช้สำหรับคลาสนี้ (รันคำสั่งเหล่านี้ใน terminal ก่อนใช้งาน)
# pip install torch pandas numpy scikit-learn

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler
from collections import deque
import logging
from datetime import datetime
from torch.cuda.amp import GradScaler, autocast
#from config import GlobalConfig

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return x

class TFTWrapper(nn.Module):
    def __init__(self, input_dim, forecast_horizon=60, d_model=64, nhead=4, num_layers=2):
        super(TFTWrapper, self).__init__()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.input_dim = input_dim
        self.forecast_horizon = forecast_horizon
        self.d_model = d_model
        self.embedding = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=0.1)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=0.1)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, input_dim)
        self.fusion_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead)
        self.scaler = RobustScaler()
        self.confidence_scores = deque(maxlen=50)
        self.multi_tf_data = {}  # เพื่อเก็บข้อมูล multi-timeframe ถ้าจำเป็น
        self.optimizer = torch.optim.AdamW(self.parameters(), lr=0.0001, weight_decay=0.01)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3)
        self.grad_scaler = GradScaler()
        self.to(self.device)

    def forward(self, x, tgt=None):
        # x: batch, seq, feature -> embed to d_model
        x = self.embedding(x)
        x = self.pos_encoder(x)
        memory = self.encoder(x)
        if tgt is None:
            tgt = torch.zeros(x.size(0), self.forecast_horizon, self.d_model).to(self.device)
        tgt = self.embedding(tgt) if tgt.shape[-1] == self.input_dim else tgt
        tgt = self.pos_encoder(tgt)
        out = self.decoder(tgt, memory)
        out = self.fc(out)
        return out

    def preprocess(self, data):
        df = pd.DataFrame(data, columns=['close', 'volume', 'RSI', 'MACD', 'RV', 'funding_rate', 'depth'])
        scaled_data = self.scaler.fit_transform(df)
        return torch.FloatTensor(scaled_data).unsqueeze(0).to(self.device)  # batch, seq, feat

    def preprocess_multi_tf(self, multi_tf_data):
        all_series = []
        for tf in GlobalConfig.get('multi_tf_list'):
            if tf in multi_tf_data and multi_tf_data[tf] is not None and not multi_tf_data[tf].empty:
                df = pd.DataFrame(multi_tf_data[tf], columns=['close', 'volume', 'RSI', 'MACD', 'RV', 'funding_rate', 'depth'])
                scaled_data = self.scaler.fit_transform(df)
                embedded = self.embedding(torch.FloatTensor(scaled_data).unsqueeze(0).to(self.device))  # batch, seq, d_model
                all_series.append(embedded)
        if all_series:
            # Fuse with attention: stack to seq_len=num_tf, batch, d_model
            stacked = torch.cat(all_series, dim=1)  # batch, total_seq, d_model
            fused, _ = self.fusion_attention(stacked.transpose(0, 1), stacked.transpose(0, 1), stacked.transpose(0, 1))
            fused = fused.transpose(0, 1)  # back to batch, seq, d_model
            return fused
        return self.preprocess(np.zeros((10, 7)))

    def predict(self, state_data):
        if isinstance(state_data, dict):
            input_tensor = self.preprocess_multi_tf(state_data)
        else:
            input_tensor = self.preprocess(state_data)
        with torch.no_grad(), autocast():
            pred = self.forward(input_tensor)
        pred_values = pred.squeeze(0).cpu().numpy()
        # Improved confidence: use MC dropout for uncertainty
        self.train()  # enable dropout
        with torch.no_grad(), autocast():
            mc_preds = [self.forward(input_tensor).squeeze(0).cpu().numpy() for _ in range(10)]
        std = np.std(mc_preds, axis=0)
        confidence = np.mean(1 / (std + 1e-6))
        self.confidence_scores.append(confidence)
        self.eval()  # back to eval
        return pred_values

    def train(self, state_batch, target_batch):
        if isinstance(state_batch, dict):
            state_tensor = self.preprocess_multi_tf(state_batch)
            target_tensor = self.preprocess_multi_tf(target_batch)
        else:
            state_tensor = self.preprocess(state_batch)
            target_tensor = self.preprocess(target_batch)
        loss_fn = nn.MSELoss()
        with autocast():
            out = self.forward(state_tensor, tgt=target_tensor[:, :-self.forecast_horizon, :])
            # Adjust shapes
            if out.shape != target_tensor.shape:
                target_tensor = target_tensor[:, :out.shape[1], :]
            loss = loss_fn(out, target_tensor)
        self.grad_scaler.scale(loss).backward()
        self.grad_scaler.step(self.optimizer)
        self.grad_scaler.update()
        self.optimizer.zero_grad()
        self.scheduler.step(loss.item())

    def update_realtime(self, ws_data, symbol):
        if symbol in ws_data:
            latest_data = np.array([[ws_data[symbol]['close'], ws_data[symbol]['volume'], 0, 0, 0,
                                     ws_data[symbol]['funding_rate'], ws_data[symbol]['depth']]])
            # Online fine-tuning with small batch
            self.train(latest_data, latest_data)  # simple update
            logging.debug(f"อัพเดท TFT เรียลไทม์สำหรับ {symbol}")
